#Day086 - code challenge Restaurant Reviews
"""Day086_code_challenge_Restaurant_Reviews.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bkgpFUfQsj-E42VbIOUKT-msVGN7nUv7
"""

import numpy as np
import pandas as pd
df = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\t', quoting = 3) #tsv file - \t or tab to read it

df.columns.to_list()
df.shape
df.head()

features = df['Review']
labels = df['Liked']

#Applying train_test_split on dataset
from sklearn.model_selection import train_test_split
features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2, random_state=42)

#Creating "bag of words model" with Tfidfvectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

tf = TfidfVectorizer(min_df=5) #min_df --> to add those words which are present in features atleast more than min-df

features_train_vectorized = tf.fit_transform(features_train) #fitiing the trained data

features_train_vectorized.shape

features_train_vectorized = features_train_vectorized.toarray() #converting to numpy array

features_train_vectorized.shape

features_train_vectorized[0:2] #1st and 2nd row

#Training and classificatin using Deep learning model
from keras.models import Sequential #Sequential - model
from keras.layers.core import Dense, Dropout, Activation #dense - layering/layers
from keras.optimizers import Adadelta, Adam, RMSprop
from keras.utils import np_utils

model = Sequential()
model.add(Dense(200, input_shape=(291,))) #nodes - 200, input_shape - input layer/total words in bag of model/vectorized features shape
model.add(Activation('relu')) #relu - algorithm
model.add(Dropout(0.5))

#2nd layer
model.add(Dense(100)) #nodes - 100
model.add(Activation('relu')) #relu activation
model.add(Dropout(0.5))

#3rd layer
model.add(Dense(40)) #nodes - 40
model.add(Activation('relu')) #relu activation
model.add(Dropout(0.5))

#4th layer
model.add(Dense(10)) #nodes - 10
model.add(Activation('relu')) #relu activation
model.add(Dropout(0.5))

#5th layer/output layer/last layer
model.add(Dense(1)) #nodel - 1, for output layer
model.add(Activation('sigmoid')) #sigmoid activation in output/binary classification

model.compile(loss='binary_crossentropy', optimizer = 'adam') #adam - optimizer and loss is binary_crossentropy

model.fit(features_train_vectorized, labels_train, batch_size = 100, epochs = 20) #fitting the model

#predictions
"""
Please use instead:* "np.argmax(model.predict(x), axis=-1)",   if your model does multi-class classification   
(e.g. if it uses a 'softmax' last-layer activation).* "(model.predict(x) > 0.5).astype("int32")",   if your model does binary classification
(e.g. if it uses a 'sigmoid' last-layer activation)
"""
labels_train_prediction = (model.predict(features_train_vectorized) > 0.5).astype("int32")
labels_test_prediction = (model.predict((tf.transform(features_test).toarray()))> 0.5).astype("int32")

print(list(zip(labels_train_prediction[0:5], labels_train[0:5])))
print(list(zip(labels_test_prediction[0:5], labels_test[0:5])))

#print(len(labels_train))
#print(len(labels_train_prediction))
#print(len(labels_test))
#print(len(labels_test_prediction))

#Accuracy/score
from sklearn.metrics import accuracy_score
print('Training score is:', accuracy_score(labels_train, labels_train_prediction)*100,'%')
print('Testing score is:', accuracy_score(labels_test, labels_test_prediction)*100,'%')

#Accuracy by confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(labels_train, labels_train_prediction)

score_train = (cm[0][0] + cm[1][1])/cm.sum()

cm2 = confusion_matrix(labels_test, labels_test_prediction)

score_test = (cm2[0][0] + cm2[1][1])/cm2.sum()

print('confusion matrix for the training set is:\n',cm)
print('Training Accuracy for the model is:',score_train*100,'%')

print('confusion matrix for the testing set is:\n',cm2)
print('Testing Accuracy for the model is:',score_test*100,'%')

#Testing for a review data sample
data = ['This restaurant has less number of workers which makes customers wait for longer than expected.'] #give data as alist

data = tf.transform(data) #sparse matrix type - convert to numpy array - use toarray()/todense()

data = data.toarray()

prediction = (model.predict(data) > 0.5).astype("int32")

print(prediction[0][0]) #wrong prediction is coming, more datasets may be required.



