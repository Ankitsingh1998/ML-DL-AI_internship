{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bonus_Day005_Day006_Day007_CNN_Pokemon.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "005lwy-xyI2J",
        "outputId": "3192ffef-de11-4696-8fe7-0ec282a1514d"
      },
      "source": [
        "#First mount\n",
        "import os\n",
        "print(os.getcwd())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edrstjddAB5k"
      },
      "source": [
        "#CNN model building\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Moj1fC9MAZhs"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "#1st layer\n",
        "model.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu')) #Step1 & Step2 - convolution and relu activation function\n",
        "model.add(MaxPooling2D(pool_size = (2, 2))) #Step3 - MaxPooling\n",
        "\n",
        "#2nd layer\n",
        "model.add(Conv2D(32, (3, 3), activation = 'relu')) #Step1 & Step2 - convolution and relu activation function\n",
        "model.add(MaxPooling2D(pool_size = (2, 2))) #Step3 - MaxPooling\n",
        "\n",
        "model.add(Flatten()) #Step4 - Flattening\n",
        "\n",
        "model.add(Dense(units = 128, activation = 'relu')) #64p + 64p = 128p => 128 total input nodes\n",
        "model.add(Dense(units = 3, activation = 'softmax')) #depth - 3 => output node - 3"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6h3FdRhtCQ0O"
      },
      "source": [
        "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy']) #compiling CNN with adam optimizer"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUqXWfUaC3lw",
        "outputId": "366a9184-19fc-4e39-b665-5641f00e5040"
      },
      "source": [
        "model.summary() #Summary of our model"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 62, 62, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 31, 31, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 29, 29, 32)        9248      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               802944    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3)                 387       \n",
            "=================================================================\n",
            "Total params: 813,475\n",
            "Trainable params: 813,475\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xm8lHlxDC-xU",
        "outputId": "b58a7647-526b-411b-e85f-b1777ba2bc01"
      },
      "source": [
        "#Fitting our CNN model to the images\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_data_generate = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = True)\n",
        "\n",
        "test_data_generate = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "#now we have to add the shortcut to the Drive of our Data folder in My Drive then use below one: importing dataset folders\n",
        "train_set = train_data_generate.flow_from_directory('/content/drive/MyDrive/Forsk coding school code practices/Image Datasets/training_set', target_size = (64, 64), batch_size = 32, class_mode = 'categorical')\n",
        "\n",
        "test_set = test_data_generate.flow_from_directory('/content/drive/MyDrive/Forsk coding school code practices/Image Datasets/test_set', target_size = (64, 64), batch_size = 32, class_mode = 'categorical')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 445 images belonging to 3 classes.\n",
            "Found 238 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CcQ5oOqEORV",
        "outputId": "c4c2a443-e465-415d-923d-da35ffbc869f"
      },
      "source": [
        "print(type(train_set))\n",
        "print(type(train_data_generate))\n",
        "print(type(test_set))\n",
        "print(type(test_data_generate))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'keras.preprocessing.image.DirectoryIterator'>\n",
            "<class 'keras.preprocessing.image.ImageDataGenerator'>\n",
            "<class 'keras.preprocessing.image.DirectoryIterator'>\n",
            "<class 'keras.preprocessing.image.ImageDataGenerator'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8z2Z5EAIEqcX",
        "outputId": "3008d1f6-a3ca-4e77-a027-05164289b866"
      },
      "source": [
        "model.fit_generator(train_set, steps_per_epoch = 10, epochs = 5, validation_data = test_set, validation_steps = 200) #steps per epoch - one batch size will get repeated this much time in one epoch"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            " 4/10 [===========>..................] - ETA: 5s - loss: 0.0706 - accuracy: 0.9766"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.9779WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 200 batches). You may need to use the repeat() function when building your dataset.\n",
            "10/10 [==============================] - 22s 2s/step - loss: 0.0643 - accuracy: 0.9779 - val_loss: 0.2257 - val_accuracy: 0.9202\n",
            "Epoch 2/5\n",
            "10/10 [==============================] - 11s 1s/step - loss: 0.0803 - accuracy: 0.9656\n",
            "Epoch 3/5\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.1161 - accuracy: 0.9590\n",
            "Epoch 4/5\n",
            "10/10 [==============================] - 11s 1s/step - loss: 0.0923 - accuracy: 0.9621\n",
            "Epoch 5/5\n",
            "10/10 [==============================] - 11s 1s/step - loss: 0.0706 - accuracy: 0.9653\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0c1866de10>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JEtNYykOnUF"
      },
      "source": [
        "***Using the Pretrained Model: VGG16***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQJlwIu8OmNV"
      },
      "source": [
        "from tensorflow.keras.applications import VGG16"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-4LXyhjPev_"
      },
      "source": [
        "conv_base = VGG16(weights = 'imagenet', include_top = False, input_shape = (150, 150, 3)) #include_top=False, means ANN part excluded and CNN part included"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3KL913SPg0f",
        "outputId": "607a1bcc-d56c-463d-8530-8b1bfa5cdb4d"
      },
      "source": [
        "conv_base.summary()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 150, 150, 3)]     0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqawVu-SPip-"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "import os\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(3, activation='softmax'))"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4A0YBE8QPk1W"
      },
      "source": [
        "conv_base.trainable = False #do not train the conv-base"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhFITWc0Pmyf"
      },
      "source": [
        "base_dir = '/content/drive/MyDrive/Forsk coding school code practices/Image Datasets'\n",
        "train_dir = os.path.join(base_dir, 'training_set')\n",
        "\n",
        "test_dir = os.path.join(base_dir, 'test_set')"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz48XLMVPqkO"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=40, width_shift_range=0.2,\n",
        "                                   height_shift_range=0.2,\n",
        "                                   shear_range=0.2,\n",
        "                                   zoom_range=0.2,\n",
        "                                   horizontal_flip=True,\n",
        "                                   fill_mode='nearest')\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255) #rescale factor is feature scalling, convert values in the range of 0-1 before applying any processing"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPv5b-wUPrCS",
        "outputId": "54ba505d-a729-4eaf-91fe-4b7ecbedeae3"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(train_dir, \n",
        "                                                    target_size=(150, 150),\n",
        "                                                    batch_size=20,\n",
        "                                                    class_mode='categorical')"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 445 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW_588SDPtNX",
        "outputId": "008ff1b6-bfab-4d78-d244-169dece2cb3f"
      },
      "source": [
        "test_generator = test_datagen.flow_from_directory(test_dir,\n",
        "                                                  target_size=(150, 150),\n",
        "                                                  batch_size=20,\n",
        "                                                  class_mode='categorical')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 238 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRscmP6OP16Q",
        "outputId": "1466ef50-3a30-49da-e72c-fb636eb7c42b"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer = optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4uzQMptP4jm",
        "outputId": "ced3cfc5-bbe0-4ba9-96a9-456b1a68eaf7"
      },
      "source": [
        "history = model.fit_generator(train_generator, steps_per_epoch=10,\n",
        "                              epochs=5,\n",
        "                              validation_data=test_generator,\n",
        "                              validation_steps=50)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "10/10 [==============================] - ETA: 0s - loss: 0.7692 - acc: 0.7550WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n",
            "10/10 [==============================] - 105s 11s/step - loss: 0.7692 - acc: 0.7550 - val_loss: 0.7500 - val_acc: 0.7437\n",
            "Epoch 2/5\n",
            "10/10 [==============================] - 51s 5s/step - loss: 0.6742 - acc: 0.8350\n",
            "Epoch 3/5\n",
            " 5/10 [==============>...............] - ETA: 25s - loss: 0.6216 - acc: 0.8500"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 48s 5s/step - loss: 0.6375 - acc: 0.8216\n",
            "Epoch 4/5\n",
            "10/10 [==============================] - 51s 5s/step - loss: 0.5919 - acc: 0.8250\n",
            "Epoch 5/5\n",
            "10/10 [==============================] - 51s 5s/step - loss: 0.5483 - acc: 0.8500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpEuETH8P9Dv"
      },
      "source": [
        "***Using the pretrained model: Fine tuning the model***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFE8DLuPQHwu"
      },
      "source": [
        "Another widely used technique for model reuse, complementary to feature extraction, is fine-tuning (see figure 5.19). \n",
        "Fine-tuning consists of unfreezing a few of the top layers of a frozen model base used for feature extraction, and jointly training both the newly added part of the model (in this case, the fully connected classifier) and these top layers. This is called ***fine-tuning*** because it slightly adjusts the more\n",
        "abstract representations of the model being reused, in order to make them more relevant for the problem at hand."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvByforeQKJR"
      },
      "source": [
        "Thus the steps for fine-tuning a network are as follows:\n",
        "\n",
        "1.   Add your custom network on top of an already-trained base network.\n",
        "2.   Freeze the base network.\n",
        "3.   Train the part you added.\n",
        "4.   Unfreeze some layers in the base network.\n",
        "5.   Jointly train both these layers and the part you added.\n",
        "\n",
        "You already completed the first three steps when doing feature extraction. Let’s proceed\n",
        "with step 4: you’ll unfreeze your conv_base and then freeze individual layers\n",
        "inside it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4lOUrx8QM6X"
      },
      "source": [
        "*You’ll* fine-tune the last three convolutional layers, which means all layers up to block4_pool should be frozen, and the layers block5_conv1, block5_conv2, and block5_conv3 should be trainable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ip5umjTHQ9og"
      },
      "source": [
        "conv_base.trainable = True\n",
        "set_trainable = False\n",
        "for layer in conv_base.layers:\n",
        "  if layer.name == 'block5_conv1':\n",
        "    set_trainable = True\n",
        "    \n",
        "  if set_trainable:\n",
        "    layer.trainable = True\n",
        "  else:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkGzJN5zRABJ",
        "outputId": "f10797bb-82ba-4ccd-d2b1-8f7057501c6a"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=10,\n",
        "    epochs=5,\n",
        "    validation_data=test_generator,\n",
        "    validation_steps=50)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - ETA: 0s - loss: 0.4347 - acc: 0.8750WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n",
            "10/10 [==============================] - 133s 14s/step - loss: 0.4347 - acc: 0.8750 - val_loss: 0.4260 - val_acc: 0.8571\n",
            "Epoch 2/5\n",
            "10/10 [==============================] - 50s 5s/step - loss: 0.4090 - acc: 0.8650\n",
            "Epoch 3/5\n",
            "10/10 [==============================] - 51s 5s/step - loss: 0.3699 - acc: 0.9050\n",
            "Epoch 4/5\n",
            "10/10 [==============================] - 50s 5s/step - loss: 0.3550 - acc: 0.8900\n",
            "Epoch 5/5\n",
            "10/10 [==============================] - 49s 5s/step - loss: 0.3412 - acc: 0.9000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kch1PCdRRB3Y"
      },
      "source": [
        "Here’s what you should take away from the exercises in the past two sections:\n",
        "\n",
        "1. Convnets are the best type of machine-learning models for computer-vision\n",
        "tasks. It’s possible to train one from scratch even on a very small dataset, with decent results.\n",
        "\n",
        "2. On a small dataset, overfitting will be the main issue. Data augmentation is a powerful way to fight overfitting when you’re working with image data.\n",
        "\n",
        "3. It’s easy to reuse an existing convnet on a new dataset via feature extraction. This is a valuable technique for working with small image datasets.\n",
        "\n",
        "4. As a complement to feature extraction, you can use fine-tuning, which adapts to a new problem some of the representations previously learned by an existing\n",
        "model. This pushes performance a bit further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBRrGEyvRPUB",
        "outputId": "2b719731-a3c5-4d4a-a0f6-df74efe2bdd5"
      },
      "source": [
        "divmod(23, 5) #returns a tuple => (quotient, remainder)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    }
  ]
}