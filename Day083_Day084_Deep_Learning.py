#Day083, Day084 - Balanced Reviews DL
"""Day083.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lJNZHTgruh98cuMXcs8H-Q9rSC0dmPlF
"""

"""
from google.colab import drive
drive.mount('/content/drive/')
"""

import pandas as pd

df = pd.read_csv('Balanced_reviews.csv')

df.head(5)

df.isnull().any(axis=0)
df.dropna(inplace=True)
df.shape

df = df[df['overall'] != 3]
df.shape
df['overall'].value_counts()

import numpy as np
df['positivity'] = np.where(df['overall'] > 3, 1, 0)
df.shape

df['positivity'].value_counts()

features = df['reviewText']
labels = df['positivity']

from sklearn.model_selection import train_test_split
features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size = 0.5, random_state=42)

#Tf-idf : Term frequency - inverse document frequency
from sklearn.feature_extraction.text import TfidfVectorizer
vect = TfidfVectorizer(min_df=5, max_features=3000).fit(features_train)
#min_df means ignore the terms/words that appears less than the set value, by default it is 5. 

len(vect.vocabulary_) #to get total number of vaocabulary created

len(vect.get_feature_names())
vect.get_feature_names()[2561:2566]

features_train_vectorized = vect.transform(features_train)
features_train_vectorized = features_train_vectorized.todense()

features_train_vectorized

#Hidden layers : 100, 500, 50
#IL Nodes: 5000 - max_features/total features
#OL Nodes: 1 single node with sigmoid activation - classification/binary

#Deep Learning modules
import numpy as np
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.optimizers import Adadelta, Adam, RMSprop
from keras.utils import np_utils

#Deep layer model building in keras
#del model

model = Sequential()

model.add(Dense(1000, input_shape=(3000,))) #First hidden layer with input layer

model.add(Activation('relu')) #ReLu activation function

model.add(Dropout(0.5)) 

model.add(Dense(500)) #Second hidden layer

model.add(Activation('relu')) #ReLu a.f.

model.add(Dropout(0.5))

model.add(Dense(50)) #Third hidden layer

model.add(Activation('relu')) #ReLu a.f.




model.add(Dropout(0.5))

model.add(Dense(1)) #Output layer

model.add(Activation('sigmoid')) #sigmoid function - binary classification - OL

model.compile(loss='binary_crossentropy', optimizer='adam') #adam optimizer/algorithm to update wieght

model.fit(features_train_vectorized, labels_train, batch_size=32, epochs=10) #model fitting

#Model prediction
labels_train_predictionclass = model.predict_classes(features_train_vectorized,batch_size=64)

labels_test_predictionclass = model.predict_classes(vect.transform(features_test).todense(),batch_size=64)

from sklearn.metrics import accuracy_score

print (("nnDeep Neural Network - Train accuracy:"),(round(accuracy_score( labels_train, labels_train_predictionclass),3)))

print (("nDeep Neural Network - Test accuracy:"),(round(accuracy_score( labels_test,labels_test_predictionclass),3)))

model.save('dl_model.h5')

#how to load the keras model
from keras.models import load_model
recreated_model = load_model('dl_model.h5')

recreated_model.summary()

"""
Dropout is a technique where randomly selected neurons are ignored during training. 
They are “dropped-out” randomly. This means that their contribution to the activation 
of downstream neurons is temporally removed on the forward pass and any weight updates 
are not applied to the neuron on the backward pass
"""